{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada63c29",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "\n",
    "- 回归(regression)：建模自变量与因变量之间关系，表示输入与输出关系。通常执行预测(prediction)任务。如预测价格、预测需求量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e41af7",
   "metadata": {},
   "source": [
    "## 线性回归基本元素\n",
    "- training data, or training set\n",
    "- 每行数据(sample/data point/data instance)\n",
    "- 预测目标（如预测房价）(label/target)\n",
    "- 预测所依据的自变量（如面积、房龄等）(feature/covariate)\n",
    "- 偏置量是当所有参数为0时，预测值应该是多少(bias/offset/intercept)，若没有会限制模型表达能力\n",
    "- 权重为自变量对目标变量的影响程度(weight)\n",
    "\n",
    "### 线性模型\n",
    "- target为房屋价格\n",
    "- feature为房龄和面积\n",
    "- weight为$\\omega_{area}$和$\\omega_{age}$\n",
    "- bias为$b$\n",
    "  $$ price=\\omega_{area} \\cdot area+\\omega_{age} \\cdot age+b$$\n",
    "- 这个模型是输入特征的一个仿射变换(affine transformation)，其特点是通过weight和对feature进行线性变换(linear transformation)，并通过bias来进行平移(translation)\n",
    "  \n",
    "#### 也就是说\n",
    "在机器学习中，预测结果$\\hat{y}$表示为：\n",
    "$$ \\hat{y}=\\omega_1 x_1+\\omega_2 x_2+\\cdots+\\omega_n x_n+b=\\omega^\\top x+b $$\n",
    "\n",
    "当$\\mathbf{X}$ 每一行是一个sample，每一列是一个feature时，即 $\\mathbf{X}=\\begin{bmatrix}x_1^T\\\\x_2^T\\\\\\vdots\\\\x_n^\\top\\end{bmatrix}$时，\n",
    "$$\\hat{y}=\\mathbf{X}\\mathbf{\\omega}+b$$\n",
    "\n",
    "- 这个过程中求和使用broadcast  \n",
    "- 当给定training data feature $\\mathbf{X}$, label $y$ 时，线性回归的target是找到一组weight $\\mathbf{\\omega}$和bias $b$，这组数据能使新sample的预测结果尽量接近真实值\n",
    "- 在寻找model parameters($\\omega$和$b$)之前，我们还需要\n",
    "  1. 模型质量的度量方式\n",
    "  2. 能更新模型的方法，以达到更高模型质量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01f1de",
   "metadata": {},
   "source": [
    "### 损失函数(loss function)\n",
    "- 量化实际值与预测值的差距\n",
    "- 通常选用正数作为损失，越小越好\n",
    "- 回归问题中最常用的时平方误差函数\n",
    "  $$ l^{(i)}(\\omega,b)=\\frac{1}{2}(y^{(i)}-\\hat{y}^{(i)})^2 $$\n",
    "- 以n个样本的损失均值作为损失函数\n",
    "  $$ L(\\omega,b)=\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\omega,b) = \\frac{1}{n}\\sum_{i=1}^n (y^{(i)}-\\omega^\\top x^{(i)}-b)^2$$\n",
    "- 我们的目的是寻找一组parameters $\\omega^*,b^*$，使得损失函数最小化\n",
    "  $$ \\omega^*,b^*=\\arg\\min_{\\omega,b} L(\\omega,b) $$\n",
    "\n",
    "### 解析解\n",
    "- 线性回归的解可以用一个公式表达出来，称为解析解(analytical solution)\n",
    "- 将$b$ 合并到$\\omega$中，即$\\hat{y}$的增广（augment matrix），具体过程为：\n",
    "  1. 合并\n",
    "  $$\n",
    "  \\mathbf{X}=\\begin{bmatrix}\n",
    "  \\mathbf{X}&|&1\n",
    "  \\end{bmatrix}\n",
    "  =\\begin{bmatrix}\n",
    "  X_1&|&1\\\\\n",
    "  X_2&|&1\\\\\n",
    "  \\vdots&|&\\vdots\\\\\n",
    "  X_n&|&1\n",
    "  \\end{bmatrix}, \n",
    "  \\omega=\\begin{bmatrix}\n",
    "  \\omega_1\\\\\n",
    "  \\omega_2\\\\\n",
    "  \\vdots\\\\\n",
    "  \\omega_n\\\\\n",
    "  \\hline\\\\\n",
    "  b\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\hat{y}=\\mathbf{X}\\cdot \\omega=\\begin{bmatrix}\n",
    "  X_1 \\omega\\\\\n",
    "  X_2 \\omega\\\\\n",
    "  \\vdots\\\\\n",
    "  X_n \\omega\n",
    "  \\end{bmatrix}\\\\\n",
    "  $$\n",
    "  2. 定义损失函数\n",
    "  $$\n",
    "  \\Rightarrow\n",
    "  L=\\|\\hat{y}-y\\|^2=(\\mathbf{X}\\omega-y)^\\top (\\mathbf{X}\\omega-y)=\\omega^\\top \\mathbf{X}^\\top \\mathbf{X}\\omega-\\omega^\\top \\mathbf{X}^\\top y-y^\\top \\mathbf{X}\\omega+y^\\top y\\\\\n",
    "  \n",
    "  $$\n",
    "  3. 求导：根据ch2:4.calculus.ipynb中梯度内容可得\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\Rightarrow\n",
    "  \\frac{\\partial L}{\\partial \\omega}&=\\frac{\\partial (\\omega^\\top \\mathbf{X}^\\top \\mathbf{X}\\omega)}{\\partial \\omega}-\\frac{\\partial (\\omega^\\top \\mathbf{X}^\\top y)}{\\partial \\omega}-\\frac{\\partial (y^\\top \\mathbf{X}\\omega)}{\\partial \\omega}-\\frac{\\partial (y^\\top y)}{\\partial \\omega}\\\\\n",
    "  &=2\\mathbf{X}^\\top \\mathbf{X}\\omega-\\mathbf{X}^\\top y-\\mathbf{X}^\\top y=2\\mathbf{X}^\\top \\mathbf{X}\\omega-2\\mathbf{X}^\\top y\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "  4. 求解loss function最小值：\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\omega}=0\\Rightarrow 2\\mathbf{X}^\\top \\mathbf{X}\\omega-2\\mathbf{X}^\\top y=0\n",
    "  \\Rightarrow \\mathbf{X}^\\top \\mathbf{X}\\omega= \\mathbf{X}^\\top y$$\n",
    "    - 若$\\mathbf{X}^\\top \\mathbf{X}$可逆\n",
    "  $$\\Rightarrow \\omega^*=(X^\\top X)^{-1}X^Ty$$\n",
    "- 并不是所有问题都有解析解\n",
    "\n",
    "\n",
    "### 随机梯度下降\n",
    "- 即使没有解析解，也能有效训练模型\n",
    "- 梯度下降(gradient descent)：计算loss function对于"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649330e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
