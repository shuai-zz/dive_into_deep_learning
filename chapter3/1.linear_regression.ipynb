{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada63c29",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "\n",
    "- 回归(regression)：建模自变量与因变量之间关系，表示输入与输出关系。通常执行预测(prediction)任务。如预测价格、预测需求量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e41af7",
   "metadata": {},
   "source": [
    "## 线性回归基本元素\n",
    "- training data, or training set\n",
    "- 每行数据(sample/data point/data instance)\n",
    "- 预测目标（如预测房价）(label/target)\n",
    "- 预测所依据的自变量（如面积、房龄等）(feature/covariate)\n",
    "- 偏置量是当所有参数为0时，预测值应该是多少(bias/offset/intercept)，若没有会限制模型表达能力\n",
    "- 权重为自变量对目标变量的影响程度(weight)\n",
    "\n",
    "### 线性模型\n",
    "- target为房屋价格\n",
    "- feature为房龄和面积\n",
    "- weight为$\\omega_{area}$和$\\omega_{age}$\n",
    "- bias为$b$\n",
    "  $$ price=\\omega_{area} \\cdot area+\\omega_{age} \\cdot age+b$$\n",
    "- 这个模型是输入特征的一个仿射变换(affine transformation)，其特点是通过weight和对feature进行线性变换(linear transformation)，并通过bias来进行平移(translation)\n",
    "  \n",
    "#### 也就是说\n",
    "在机器学习中，预测结果$\\hat{y}$表示为：\n",
    "$$ \\hat{y}=\\omega_1 x_1+\\omega_2 x_2+\\cdots+\\omega_n x_n+b=\\omega^\\top x+b $$\n",
    "\n",
    "当$\\mathbf{X}$ 每一行是一个sample，每一列是一个feature时，即 $\\mathbf{X}=\\begin{bmatrix}x_1^T\\\\x_2^T\\\\\\vdots\\\\x_n^\\top\\end{bmatrix}$时，\n",
    "$$\\hat{y}=\\mathbf{X}\\mathbf{\\omega}+b$$\n",
    "\n",
    "- 这个过程中求和使用broadcast  \n",
    "- 当给定training data feature $\\mathbf{X}$, label $y$ 时，线性回归的target是找到一组weight $\\mathbf{\\omega}$和bias $b$，这组数据能使新sample的预测结果尽量接近真实值\n",
    "- 在寻找model parameters($\\omega$和$b$)之前，我们还需要\n",
    "  1. 模型质量的度量方式\n",
    "  2. 能更新模型的方法，以达到更高模型质量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01f1de",
   "metadata": {},
   "source": [
    "### 损失函数(loss function)\n",
    "- 量化实际值与预测值的差距\n",
    "- 通常选用正数作为损失，越小越好\n",
    "- 回归问题中最常用的时平方误差函数\n",
    "  $$ l^{(i)}(\\omega,b)=\\frac{1}{2}(y^{(i)}-\\hat{y}^{(i)})^2 $$\n",
    "- 以n个样本的损失均值作为损失函数\n",
    "  $$ L(\\omega,b)=\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\omega,b) = \\frac{1}{n}\\sum_{i=1}^n (y^{(i)}-\\omega^\\top x^{(i)}-b)^2$$\n",
    "- 我们的目的是寻找一组parameters $\\omega^*,b^*$，使得损失函数最小化\n",
    "  $$ \\omega^*,b^*=\\arg\\min_{\\omega,b} L(\\omega,b) $$\n",
    "\n",
    "### 解析解\n",
    "- 线性回归的解可以用一个公式表达出来，称为解析解(analytical solution)\n",
    "- 将$b$ 合并到$\\omega$中，即$\\hat{y}$的增广（augment matrix），具体过程为：\n",
    "  1. 合并\n",
    "  $$\n",
    "  \\mathbf{X}=\\begin{bmatrix}\n",
    "  \\mathbf{X}&|&1\n",
    "  \\end{bmatrix}\n",
    "  =\\begin{bmatrix}\n",
    "  X_1&|&1\\\\\n",
    "  X_2&|&1\\\\\n",
    "  \\vdots&|&\\vdots\\\\\n",
    "  X_n&|&1\n",
    "  \\end{bmatrix}, \n",
    "  \\omega=\\begin{bmatrix}\n",
    "  \\omega_1\\\\\n",
    "  \\omega_2\\\\\n",
    "  \\vdots\\\\\n",
    "  \\omega_n\\\\\n",
    "  \\hline\\\\\n",
    "  b\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\hat{y}=\\mathbf{X}\\cdot \\omega=\\begin{bmatrix}\n",
    "  X_1 \\omega\\\\\n",
    "  X_2 \\omega\\\\\n",
    "  \\vdots\\\\\n",
    "  X_n \\omega\n",
    "  \\end{bmatrix}\\\\\n",
    "  $$\n",
    "  2. 定义损失函数\n",
    "  $$\n",
    "  \\Rightarrow\n",
    "  L=\\|\\hat{y}-y\\|^2=(\\mathbf{X}\\omega-y)^\\top (\\mathbf{X}\\omega-y)=\\omega^\\top \\mathbf{X}^\\top \\mathbf{X}\\omega-\\omega^\\top \\mathbf{X}^\\top y-y^\\top \\mathbf{X}\\omega+y^\\top y\\\\\n",
    "  \n",
    "  $$\n",
    "  3. 求导：根据ch2:4.calculus.ipynb中梯度内容可得\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\Rightarrow\n",
    "  \\frac{\\partial L}{\\partial \\omega}&=\\frac{\\partial (\\omega^\\top \\mathbf{X}^\\top \\mathbf{X}\\omega)}{\\partial \\omega}-\\frac{\\partial (\\omega^\\top \\mathbf{X}^\\top y)}{\\partial \\omega}-\\frac{\\partial (y^\\top \\mathbf{X}\\omega)}{\\partial \\omega}-\\frac{\\partial (y^\\top y)}{\\partial \\omega}\\\\\n",
    "  &=2\\mathbf{X}^\\top \\mathbf{X}\\omega-\\mathbf{X}^\\top y-\\mathbf{X}^\\top y=2\\mathbf{X}^\\top \\mathbf{X}\\omega-2\\mathbf{X}^\\top y\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "  4. 求解loss function最小值：\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\omega}=0\\Rightarrow 2\\mathbf{X}^\\top \\mathbf{X}\\omega-2\\mathbf{X}^\\top y=0\n",
    "  \\Rightarrow \\mathbf{X}^\\top \\mathbf{X}\\omega= \\mathbf{X}^\\top y$$\n",
    "    - 若$\\mathbf{X}^\\top \\mathbf{X}$可逆\n",
    "  $$\\Rightarrow \\omega^*=(X^\\top X)^{-1}X^Ty$$\n",
    "- 并不是所有问题都有解析解\n",
    "\n",
    "\n",
    "### 随机梯度下降\n",
    "- 即使没有解析解，也能有效训练模型\n",
    "- 梯度下降(gradient descent)：计算loss function对于model parameters的导数，并更新参数，但可能非常慢，因为需要遍历整个数据集\n",
    "- 因此在需要更新时随机抽取一批样本，称为小批量随机梯度下降(minibatch stochastic gradient descent)。即在每次迭代中随机抽样固定数量小批量$\\beta$，计算小样本梯度，最后将梯度乘以一个预先确定的正数$\\eta$，并从当前参数的值中减去。\n",
    "  $$(\\omega,b)\\leftarrow(\\omega,b)-\\frac{\\eta}{|\\beta|}\\sum_{i\\in\\beta} \\partial_{(\\omega,b)} l^{(i)}(\\omega,b)$$\n",
    "\n",
    "- 总结\n",
    "  - loss function：平方损失\n",
    "  - 算法步骤：\n",
    "    1. 初始化模型参数的值，如随机初始化\n",
    "    2. 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代。\n",
    "  - 对于平方损失和仿射变换，即为如下形式\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\omega &\\leftarrow \\omega-\\frac{\\eta}{|\\beta|} \\sum_{i\\in\\beta} \\partial_\\omega l^{(i)}(\\omega,b)\n",
    "    =\\omega-\\frac{\\eta}{|\\beta|} \\sum_{i\\in\\beta}x^{(i)}(\\omega^\\top x^{(i)}+b-y^{(i)}) \\\\\n",
    "    b &\\leftarrow b-\\frac{\\eta}{|\\beta|} \\sum_{i\\in\\beta}\\partial_b l^{(i)}(\\omega,b)=b-\\frac{\\eta}{|\\beta|} \\sum_{i\\in\\beta} (\\omega^\\top x^{(i)}+b-y^{(i)})\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "  - 其中$\\omega$和$x$都是向量\n",
    "  - $|\\beta|$是小批量样本数(batch size), $\\eta$是学习率(learning rate)，通常手动指定。这种手动指定的参数叫超参数(hyperparameter)，通过在验证数据集(validation dataset)上得到的训练迭代结果调整参数(hyperparameter tuning)\n",
    "  - 训练结束后得到$\\hat{\\omega},\\hat{b}$，但是这些估计值不会使loss function最小化，因为是向最小值缓慢收敛，而不是在有限步数内精确得到最小值\n",
    "  - linear regression恰好只有一个最小值，但深度神经网络等复杂模型往往有多个最小值，所有挑战为找到一组参数，能在没见过的数据上实现较低损失，这一挑战即为泛化(generalization)\n",
    "\n",
    "### 用模型进行预测\n",
    "给定特征估计目标的过程称为预测(prediction)或推断(inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649330e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
