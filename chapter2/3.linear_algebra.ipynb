{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447ec50c",
   "metadata": {},
   "source": [
    "# 线性代数\n",
    "## 标量\n",
    "- scalar：数，由只有一个元素的tensor表示\n",
    "- variable：变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c03b8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.tensor(3.0)\n",
    "y=torch.tensor(2.0)\n",
    "x+y,x*y,x/y,x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80eb77",
   "metadata": {},
   "source": [
    "## 向量\n",
    "- 由scalar组成的序列\n",
    "- 每个scalar叫做element 或 component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f98c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor(3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(4)\n",
    "x,x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224624a",
   "metadata": {},
   "source": [
    "### 长度、维度和形状\n",
    "- 长度、维度：向量的scalar数量\n",
    "- 形状：m*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552e3956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, torch.Size([4]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x),x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1b1be",
   "metadata": {},
   "source": [
    "## 矩阵\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a34745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15],\n",
       "         [16, 17, 18, 19]]),\n",
       " tensor([[ 0,  4,  8, 12, 16],\n",
       "         [ 1,  5,  9, 13, 17],\n",
       "         [ 2,  6, 10, 14, 18],\n",
       "         [ 3,  7, 11, 15, 19]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.arange(20).reshape(5,4)\n",
    "A,A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f99869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Symmetric Matrix: M=M Transpose\n",
    "B=torch.tensor([[1,2,3],[2,0,4],[3,4,5]])\n",
    "B==B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb0c04",
   "metadata": {},
   "source": [
    "# 张量 \n",
    "n维矩阵(n>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43ab40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.arange(24).reshape(2,3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017e2d0",
   "metadata": {},
   "source": [
    "## 张量算法基本性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fedb3ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]),\n",
       " False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "# 通过分配新内存，将A的副本赋值给B\n",
    "B=A.clone()\n",
    "A,A+B,id(A)==id(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03875e07",
   "metadata": {},
   "source": [
    "Hadamard Product：Hardamard积\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B}=\\begin{bmatrix}\n",
    "a_{11} b_{11} & a_{12} b_{12} & \\cdots & a_{1n} b_{1n} \\\\\n",
    "a_{21} b_{21} & a_{22} b_{22} & \\cdots & a_{2n} b_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} b_{m1} & a_{m2} b_{m2} & \\cdots & a_{mn} b_{mn}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc912633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e311a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor加上或乘以一个scalar，不影响tensor的形状\n",
    "a=2\n",
    "a+X,(a*X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8358e",
   "metadata": {},
   "source": [
    "## 降维\n",
    "对tensor求和：即将所有element相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228b2da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(4,dtype=torch.float32)\n",
    "x,x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8101c997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 或任意形状tensor\n",
    "A.shape,A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6897426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况，sum()会沿所有皱降维，使其变成标量。可指定tensor的轴来实现降维\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0f60179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97937821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5000) tensor([ 8.,  9., 10., 11.]) tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000])\n"
     ]
    }
   ],
   "source": [
    "# 平均值：mean或average，也可以沿着指定轴降低tensor维度\n",
    "print(A.mean(),A.mean(axis=0),A.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd49c3a",
   "metadata": {},
   "source": [
    "### 非降维求和\n",
    "当仍保持与原tensor A一样的轴数量时，可以通过广播操作将A除以sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a783f1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.],\n",
       "         [22.],\n",
       "         [38.],\n",
       "         [54.],\n",
       "         [70.]]),\n",
       " torch.Size([5, 1]),\n",
       " tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "         [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "         [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "         [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "         [0.2286, 0.2429, 0.2571, 0.2714]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A=A.sum(axis=1,keepdim=True)\n",
    "sum_A,sum_A.shape,A/sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13664953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 沿某个轴计算A的累计总和：cumulative sum\n",
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e8c72",
   "metadata": {},
   "source": [
    "## 点积(dot product)\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \n",
    "\\begin{bmatrix} a_1 & a_2 & a_3 \\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n",
    "= a_1b_1 + a_2b_2 + a_3b_3\n",
    "$$\n",
    "- 向量a表示值，向量b表示权重 -> dot(a,b)表示加权和\n",
    "- 当权重非负数且和为一，表示加权平均值\n",
    "- 两个向量规范化得到单位长度时，点积表示夹角余弦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a34d656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.ones(4,dtype=torch.float32)\n",
    "\n",
    "x,y,torch.dot(x,y),\n",
    "# 也就是torch.sum(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94569e",
   "metadata": {},
   "source": [
    "## 矩阵向量积\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{x}=\\begin{bmatrix}\n",
    "a_1 & a_2 & \\cdots & a_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_1^T \\\\\n",
    "a_2^T\\\\\n",
    "\\vdots\\\\\n",
    "a_n^T\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "a_1^T x\\\\\n",
    "a_2^T x\\\\\n",
    "\\vdots\\\\\n",
    "a_n^T x\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2d90898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape,x.shape,torch.mv(A,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbdee9",
   "metadata": {},
   "source": [
    "## 矩阵乘法\n",
    "$$\n",
    "\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{bmatrix},\n",
    "\\mathbf{B}=\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} & \\cdots & b_{1n} \\\\\n",
    "b_{21} & b_{22} & \\cdots & b_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{m1} & b_{m2} & \\cdots & b_{mn}\n",
    "\\end{bmatrix},\n",
    "\\mathbf{C}=\n",
    "\\begin{bmatrix}\n",
    "a_1^T\\\\\n",
    "a_2^T\\\\\n",
    "\\vdots\\\\\n",
    "a_m^T\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & \\cdots & b_n\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "a_1^Tb_1 & a_1^Tb_2 & \\cdots & a_1^Tb_n \\\\\n",
    "a_2^Tb_1 & a_2^Tb_2 & \\cdots & a_2^Tb_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_m^Tb_1 & a_m^Tb_2 & \\cdots & a_m^Tb_n\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f8c4df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=torch.ones(4,3)\n",
    "torch.mm(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a7b02",
   "metadata": {},
   "source": [
    "## 范数 norm\n",
    "- 表示一个向量的大小(size)\n",
    "- 性质：\n",
    "  1. 齐次性：若用常数因子$\\alpha$乘以向量，则范数也会按照$|\\alpha|$缩放\n",
    "     $$ f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}) $$\n",
    "  2. 三角不等式：两边之和大于第三边推广\n",
    "     $$ f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}) $$\n",
    "  3. 非负性：范数必须大于0\n",
    "     $$ f(\\mathbf{x}) \\geq 0 $$\n",
    "  4. 正定性：当且仅当$\\mathbf{x}$的所有元素都为0时，范数才等于0\n",
    "     $$ f(\\mathbf{x}) = 0 \\iff \\mathbf{x} = \\mathbf{0} $$\n",
    "- $\\mathbf{L_2}$范数：向量$\\mathbf{x}$的平方和开根号，也叫欧式范数，即所有元素平方和的平方根(通常省略下标)\n",
    "  $$ \\|\\mathbf{X}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2} $$\n",
    "- $\\mathbf{L_1}$范数：向量$\\mathbf{x}$的绝对值和，也叫曼哈顿范数\n",
    "  $$ \\|\\mathbf{X}\\|_1 = \\sum_{i=1}^n |x_i| $$\n",
    "- $\\mathbf{L_p}$范数：$\\mathbf{L_1}$和$\\mathbf{L_2}$都是其特例\n",
    "  $$ \\|\\mathbf{X}\\|_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p} $$\n",
    "- 类似向量范数，矩阵的Frobenius范数是矩阵元素平方和的平方根\n",
    "  $$ \\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n X_{i,j}^2} $$\n",
    "\n",
    "## 范数和目标\n",
    "用向量表示物品（如单词，产品或新闻），以便最小化相似项目之间的距离，最大化不同项目之间的距离"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be004b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb938ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L2\n",
    "u=torch.tensor([3.0,-4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90f8a15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L1\n",
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca47c2",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c979a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 证明A转置的转置是A\n",
    "A=torch.arange(12).reshape(3,4)\n",
    "A.T.T==A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a8876c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 证明转置的和等于和的转置\n",
    "B=torch.randn(3,4)\n",
    "(A+B).T==(A.T+B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf70544",
   "metadata": {},
   "source": [
    "给定任意方阵C，C+C^T总是对称的吗？\n",
    "\n",
    "是的，因为\n",
    "$$(\\mathbf{C} + \\mathbf{C}^T)^T=\\mathbf{C}^T + \\mathbf{C}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80cb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x是(2，3，4)的tensor，len(x)的结果是什么\n",
    "X=torch.randn(2,3,4)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54864e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于任意形状的tensor，len(x)是否总是对应特定轴的长度，这个轴是什么\n",
    "# 是的，len(x)总是对应tensor的第0轴的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c782a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# A/A.sum(axis=1)结果，为什么\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mA\u001b[49m\u001b[43m/\u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#Error: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 这两个tensor在进行运算时，要求第 1 维尺寸必须相同，但当前不匹配，因此报错。\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# A/A.sum(axis=1)结果，为什么\n",
    "# A/A.sum(axis=1)\n",
    "#Error: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1\n",
    "# 这两个tensor在进行运算时，要求第 1 维尺寸必须相同，但当前不匹配，因此报错。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a4e5fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([2, 4]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个(2,3,4)的tensor，在轴0，1，2上分别求和的结果形状各是什么\n",
    "X.sum(axis=0).shape,X.sum(axis=1).shape,X.sum(axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ef158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(65.7571)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#为linalg.norm函数提供3个轴的tensor，观察其输出，得到了什么\n",
    "X=torch.arange(24,dtype=torch.float32).reshape(2,3,4)\n",
    "torch.linalg.norm(X)  \n",
    "# 输出tensor的FFrobenius范数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
